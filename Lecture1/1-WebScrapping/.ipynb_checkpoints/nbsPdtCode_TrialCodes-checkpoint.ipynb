{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBS 产品爬虫--Trial Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of retrying requesting the website\n",
    "import random\n",
    "import urllib.request\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "\n",
    "def reopen(url, timeout, maxt):\n",
    "    \"\"\"\n",
    "    This function reopens the url under random sleeping time, the sleep time increases\n",
    "    as more trials are called \n",
    "    ---------------------------------------------------------------------------------\n",
    "    Var Def:\n",
    "    url: objective url\n",
    "    timeout: maximum time allowed for opening the url\n",
    "    longsleep: the time for a long sleep after failed trials\n",
    "    maxt: maximum number of trials\n",
    "    maxpause: maximum number of pauses (long sleep)\n",
    "    \"\"\"\n",
    "    fails = 0\n",
    "    pause = 0\n",
    "    while True:\n",
    "        try:\n",
    "            if fails >= maxt:\n",
    "                #if pause >= maxpause:\n",
    "                #    print('Dead Url:'+url)\n",
    "                #    print('Number of Pauses:', maxpause)\n",
    "                #    break\n",
    "                #print('Getting a pause of ', longsleep,'s...')\n",
    "                #time.sleep(longsleep + random.random())\n",
    "                #pause +=1\n",
    "                #html = opener.open(url, None, timeout)\n",
    "                #return html\n",
    "                print('Dead Url:'+url)\n",
    "                break\n",
    "            html = opener.open(url, None, timeout)\n",
    "            return html\n",
    "        except:\n",
    "            fails +=1\n",
    "            stime = fails + random.random() # sleeping time increases as we try more\n",
    "            time.sleep(stime)\n",
    "            print ('connection timeout, retrying:', fails)\n",
    "            print ('sleeping time:', stime)\n",
    "        else: \n",
    "            print(url)\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of retrieving hrefs and texts\n",
    "#-------------------------------------------------\n",
    "def reftextRetrieve(item):\n",
    "    if item.select('a')!=[]:\n",
    "        item_a = item.select('a')\n",
    "        href = item_a[0].get('href')\n",
    "        return item_a, href\n",
    "    else:\n",
    "        if item.select('td')!=[]:\n",
    "            item_a = item.select('td')\n",
    "            href = []\n",
    "            return item_a, href\n",
    "        else:\n",
    "            print('EEOR!: selected invalid tag') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first page with two-digit products\n",
    "\n",
    "my_url = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/index.html'\n",
    "html = uReq(my_url)\n",
    "page_soup = soup(html.read(), 'html.parser')\n",
    "\n",
    "# find each name of the product catalog\n",
    "containers = page_soup.findAll(\"span\", {\"class\":\"cont_tit\"})\n",
    "code_product = []\n",
    "for item in containers:\n",
    "    item_list = item.findAll('font', {'class':'cont_tit03'})\n",
    "    name = item_list[0].text\n",
    "    code_product.append([name[0:2], name[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.3520821186896521\n",
      "code10:0119010000\n",
      "code10:0119020000\n",
      "code10:0119030000\n",
      "code10:0119040000\n",
      "code10:0119050000\n",
      "code10:0119060000\n",
      "code10:0119070000\n",
      "code10:0119080000\n",
      "code10:0119090000\n",
      "code10:0119100000\n",
      "code10:0119110000\n",
      "code10:0119120000\n",
      "code10:0119130000\n",
      "code10:0119140000\n",
      "code10:0119150000\n",
      "code10:0119160000\n",
      "code10:0119170000\n",
      "code10:0119180000\n",
      "code10:0119190000\n",
      "code10:0119200000\n",
      "code10:0119210000\n",
      "code10:0119220000\n",
      "code10:0119230000\n",
      "code10:0119240000\n",
      "code10:0119250000\n",
      "code10:0119260000\n",
      "code10:0119270000\n",
      "code10:0119280000\n",
      "code10:0119290000\n",
      "code10:0119300000\n",
      "code10:0119310000\n",
      "code10:0119320000\n",
      "code10:0119330000\n",
      "code10:0119340000\n",
      "code10:0119350000\n",
      "code10:0119360000\n",
      "code10:0119370000\n",
      "code10:0119380000\n",
      "code10:0119390000\n",
      "code10:0119400000\n",
      "code10:0119410000\n",
      "code10:0119420000\n",
      "code10:0119430000\n",
      "code10:0119440000\n",
      "code10:0119450000\n",
      "code10:0119460000\n",
      "code10:0119470000\n",
      "code10:0119480000\n",
      "code10:0119490000\n",
      "code10:0119990000\n"
     ]
    }
   ],
   "source": [
    "# reopen if timeout\n",
    "# random sleeping time after each trial\n",
    "# a long pause after a sequence of trials\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "t0 = time.time()\n",
    "# settings for the reopen() function\n",
    "#------------------------------------------------------\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "maxtr = 10\n",
    "timeout= 10\n",
    "\n",
    "#------------------------------------------------------\n",
    "# index page 1\n",
    "tags_index = [code_product[i][0] for i in range(len(code_product))] # tags for index html links\n",
    "url_index = []\n",
    "html_main = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/'\n",
    "for tag in tags_index:\n",
    "    link = html_main + tag +'.html'\n",
    "    url_index.append(link)\n",
    "\n",
    "#Page 1#\n",
    "for i, url in enumerate([url_index[0]]):\n",
    "    code2, pdt2 = code_product[i]  # two-digit products\n",
    "    html = reopen(url, maxtr, timeout)\n",
    "    page_sp = soup(html.read(), 'html.parser')\n",
    "    pdt_containers = page_sp.find_all(\"tr\", {\"class\": \"citytr\"})\n",
    "\n",
    "id = 18\n",
    "item = pdt_containers[id]\n",
    "item_a, href1 = reftextRetrieve(item)\n",
    "\n",
    "if item_a!=[]:\n",
    "   code4, pdt4 = [item_a[0].text, item_a[1].text]\n",
    "   if href1 ==[]:\n",
    "      code6, pdt6 = ['', '']\n",
    "      code8, pdt8 = ['', '']\n",
    "      code10,pdd10= ['', ''] \n",
    "      \n",
    "   else:\n",
    "       url1 = html_main + href1 #hyperlinks to subcategories of 6 digit\n",
    "       html1 = reopen(url1, maxtr, timeout)\n",
    "       page_sp1 = soup(html1.read(), 'html.parser')\n",
    "       pdt_containers1 = page_sp1.find_all(\"tr\", {\"class\": \"countytr\"})\n",
    "       for item in pdt_containers1:\n",
    "            item_a1, href2 = reftextRetrieve(item)\n",
    "            if item_a1!=[]:\n",
    "               code6, pdt6 = [item_a1[0].text, item_a1[1].text]\n",
    "               if href2==[]:\n",
    "                  code8, pdt8 = ['', '']\n",
    "                  code10,pdd10= ['', '']\n",
    "                                 \n",
    "               else: \n",
    "                   url2 = html_main + code2 +'/' + href2 # hyperlinks to 8-digit\n",
    "                   html2 = reopen(url2, maxtr, timeout)\n",
    "                   page_sp2 = soup(html2.read(), 'html.parser')\n",
    "                   pdt_containers2 = page_sp2.find_all(\"tr\", {\"class\": \"towntr\"})\n",
    "                   if pdt_containers2!=[]:\n",
    "                       for item in pdt_containers2:\n",
    "                           item_a2, href3 = reftextRetrieve(item)\n",
    "                           if item_a2!=[]:\n",
    "                               code8, pdt8 = [item_a2[0].text, item_a2[1].text]\n",
    "                               if href2 == []:\n",
    "                                 code10, pdt10 = ['', '']\n",
    "                                 \n",
    "                               else:\n",
    "                                  url3 = html_main + '/'.join([code4[0:2], code4[2:4]]) + '/'+ href3\n",
    "                                  html3 = reopen(url3, maxtr, timeout)\n",
    "                                  page_sp3 = soup(html3.read(), 'html.parser')\n",
    "                                  pdt_containers3 = page_sp3.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                                  for item in pdt_containers3:\n",
    "                                        item_a3 = item.select('td')\n",
    "                                        code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                        print('code10:' + code10)\n",
    "                                        \n",
    "                   else:\n",
    "                        code8, pdt8 = ['','']\n",
    "                        pdt_containers3 = page_sp2.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                        for item in pdt_containers3:\n",
    "                            item_a3 = item.select('td')\n",
    "                            code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                            print('code10:' + code10)\n",
    "                            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr class=\"villagetr\"><td>0119990000</td><td>其他中草药材</td><td>　包括主要用作杀虫、杀菌等用途的植物及其某部分</td></tr>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdt_containers3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reopen if timeout\n",
    "# random sleeping time after each trial\n",
    "# a long pause after a sequence of trials\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "t0 = time.time()\n",
    "# settings for the reopen() function\n",
    "#------------------------------------------------------\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "maxtr = 10\n",
    "timeout= 10\n",
    "\n",
    "#longsleep = 120  #long sleeping time\n",
    "#maxpause = 3     #permitted maximum number of pauses\n",
    "#------------------------------------------------------\n",
    "\n",
    "# index page 1\n",
    "tags_index = [code_product[i][0] for i in range(len(code_product))] # tags for index html links\n",
    "url_index = []\n",
    "html_main = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/'\n",
    "for tag in tags_index:\n",
    "    link = html_main + tag +'.html'\n",
    "    url_index.append(link)\n",
    "\n",
    "#Page 1#\n",
    "for i, url in enumerate([url_index[0]]):\n",
    "    code2, pdt2 = code_product[i]  # two-digit products\n",
    "    html = reopen(url, maxtr, timeout)\n",
    "    page_sp = soup(html.read(), 'html.parser')\n",
    "    pdt_containers = page_sp.find_all(\"tr\", {\"class\": \"citytr\"})\n",
    "    # loop over each categories of 4-digit\n",
    "    for item in pdt_containers:\n",
    "        item_a, href1 = reftextRetrieve(item)\n",
    "        if item_a!=[]:\n",
    "           code4, pdt4 = [item_a[0].text, item_a[1].text]\n",
    "           if href1 ==[]:\n",
    "              code6, pdt6 = ['', '']\n",
    "              code8, pdt8 = ['', '']\n",
    "              code10,pdd10= ['', ''] \n",
    "              writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "           else:\n",
    "               url1 = html_main + href1 #hyperlinks to subcategories of 6 digit\n",
    "               html1 = reopen(url1, maxtr, timeout)\n",
    "               page_sp1 = soup(html1.read(), 'html.parser')\n",
    "               pdt_containers1 = page_sp1.find_all(\"tr\", {\"class\": \"countytr\"})\n",
    "               for item in pdt_containers1:\n",
    "                    item_a1, href2 = reftextRetrieve(item)\n",
    "                    if item_a1!=[]:\n",
    "                       code6, pdt6 = [item_a1[0].text, item_a1[1].text]\n",
    "                       if href2==[]:\n",
    "                          code8, pdt8 = ['', '']\n",
    "                          code10,pdd10= ['', '']\n",
    "                          writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])                \n",
    "                       else: \n",
    "                           url2 = html_main + code2 +'/' + href2 # hyperlinks to 8-digit\n",
    "                           html2 = reopen(url2, maxtr, timeout)\n",
    "                           page_sp2 = soup(html2.read(), 'html.parser')\n",
    "                           pdt_containers2 = page_sp2.find_all(\"tr\", {\"class\": \"towntr\"})\n",
    "                           if pdt_containers2!=[]:\n",
    "                               for item in pdt_containers2:\n",
    "                                   item_a2, href3 = reftextRetrieve(item)\n",
    "                                   if item_a2!=[]:\n",
    "                                       code8, pdt8 = [item_a2[0].text, item_a2[1].text]\n",
    "                                       if href2 == []:\n",
    "                                         code10, pdt10 = ['', '']\n",
    "                                         writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "                                       else:\n",
    "                                          url3 = html_main + '/'.join([code4[0:2], code4[2:4]]) + '/'+ href3\n",
    "                                          html3 = reopen(url3, maxtr, timeout)\n",
    "                                          page_sp3 = soup(html3.read(), 'html.parser')\n",
    "                                          pdt_containers3 = page_sp3.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                                          for item in pdt_containers3:\n",
    "                                                item_a3 = item.select('td')\n",
    "                                                code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                                print('code10:' + code10)\n",
    "                                                writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "                           else:\n",
    "                                code8, pdt8 = ['','']\n",
    "                                pdt_containers3 = page_sp2.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                                for item in pdt_containers3:\n",
    "                                    item_a3 = item.select('td')\n",
    "                                    code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                    print('code10:' + code10)\n",
    "                                    writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "csvfile.close()\n",
    "t1 = time.time()\n",
    "dtime = t1 - t0\n",
    "print(\"Total Running Time: %.8s s\" %dtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fix-up codes for failed scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-Digit Success! Running Time: 2.536177 s\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "@ File    : nbs产品目录爬虫\n",
    "@ version : 2020-07-12\n",
    "@ author  : Zhiyuan Chen\n",
    "\n",
    "\"\"\"\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import urllib.request\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import time \n",
    "\n",
    "# python browser\n",
    "\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "\n",
    "\n",
    "#---------\n",
    "#functions\n",
    "#---------\n",
    "\n",
    "#(1) function for reopening urls\n",
    "\n",
    "def reopen(url, timeout, maxt):\n",
    "    \"\"\"\n",
    "    This function reopens the url under random sleeping time, the sleep time increases\n",
    "    as more trials are called \n",
    "    ---------------------------------------------------------\n",
    "    Var Def:\n",
    "    url: objective url\n",
    "    timeout: maximum time allowed for opening the url\n",
    "    longsleep: the time for a long sleep after failed trials\n",
    "    maxt: maximum number of trials\n",
    "    maxpause: maximum number of pauses (long sleep)\n",
    "    \n",
    "    \"\"\"\n",
    "    fails = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if fails >= maxt:\n",
    "                print('Dead Url:'+url)\n",
    "                break\n",
    "            html = opener.open(url, None, timeout)\n",
    "            return html\n",
    "        except:\n",
    "            fails +=1\n",
    "            stime = fails + random.random() # sleeping time increases as we try more\n",
    "            time.sleep(stime)\n",
    "            print ('connection timeout, retrying:', fails)\n",
    "            print ('sleeping time:', stime)\n",
    "        else: \n",
    "            print(url)\n",
    "            break \n",
    "\n",
    "#(2). function for retrieving htmls\n",
    "\n",
    "def reftextRetrieve(item):\n",
    "    if item.select('a')!=[]:\n",
    "        item_a = item.select('a')\n",
    "        href = item_a[0].get('href')\n",
    "        return item_a, href\n",
    "    else:\n",
    "        if item.select('td')!=[]:\n",
    "            item_a = item.select('td')\n",
    "            href = []\n",
    "            return item_a, href\n",
    "        else:\n",
    "            print('ERROR!: invalid tag selected') \n",
    "\n",
    "\n",
    "# settings for reopen urls\n",
    "maxtr = 5\n",
    "timeout= 15\n",
    "t0 = time.time()\n",
    "\n",
    "# 2-digit products extraction\n",
    "\n",
    "index_urls = ['http://www.stats.gov.cn/tjsj/tjbz/tjypflml/index.html']\n",
    "main_url = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/index_'\n",
    "url_body = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/'\n",
    "\n",
    "for i in range(1, 5):\n",
    "    index_urls.append(str(i).join([main_url, '.html']))    # get all the links to index pages\n",
    "\n",
    "for n, url in enumerate(index_urls):\n",
    "    html = reopen(url, maxtr, timeout)\n",
    "    page_soup = soup(opener.open(url).read(), 'html.parser')\n",
    "    containers = page_soup.findAll(\"span\", {'class':'cont_tit'})\n",
    "    code_product = []\n",
    "    for item in containers:\n",
    "        item_list = item.findAll('font', {'class':'cont_tit03'})\n",
    "        name = item_list[0].text\n",
    "        #print(name)\n",
    "        code_product.append([name[0:2], name[3:]])\n",
    "    filename = str(n+1).join(['pdt_2d_', '.csv'])\n",
    "    with open(filename, 'w', encoding = 'utf-8-sig', newline = '') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['code', 'product'])\n",
    "        writer.writerows(code_product)\n",
    "t1 = time.time()\n",
    "dt = t1-t0\n",
    "print(\"2-Digit Success! Running Time: %.8s s\" %dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlib.error import URLError, HTTPError\n",
    "def reopen(url, timeout, maxt):\n",
    "    \"\"\"\n",
    "    This function reopens the url under random sleeping time, the sleep time increases\n",
    "    as more trials are called \n",
    "    ---------------------------------------------------------\n",
    "    Var Def:\n",
    "    url: objective url\n",
    "    timeout: maximum time allowed for opening the url\n",
    "    longsleep: the time for a long sleep after failed trials\n",
    "    maxt: maximum number of trials\n",
    "    maxpause: maximum number of pauses (long sleep)\n",
    "    \n",
    "    \"\"\"\n",
    "    fails = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            if fails >= maxt:\n",
    "               break\n",
    "            html = opener.open(url, None, timeout)\n",
    "            return html\n",
    "               \n",
    "        except:\n",
    "            fails +=1\n",
    "            stime = fails + random.random() # sleeping time increases as we try more\n",
    "            time.sleep(stime)\n",
    "            print ('connection timeout, retrying:', fails)\n",
    "            print ('sleeping time:', stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HTTPError404' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f3f08701d760>\u001b[0m in \u001b[0;36mreopen\u001b[1;34m(url, timeout, maxt)\u001b[0m\n\u001b[0;32m     20\u001b[0m                \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5ba60757efdd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/42/07/02/42070299.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-f3f08701d760>\u001b[0m in \u001b[0;36mreopen\u001b[1;34m(url, timeout, maxt)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTTPError404\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HTTP Error 404'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HTTPError404' is not defined"
     ]
    }
   ],
   "source": [
    "reopen('http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/42/07/02/42070299.html', None, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) function for scraping subcategories (for code simplification and error checking)\n",
    "def nbsScrap(url_cpi, url_body, maxtr, timeout):\n",
    "    \"\"\"\n",
    "    url: the index web links to 4-digit products\n",
    "    url_body: the url body used to construct links to subpages\n",
    "    filename: filename for the csvfile: 'name.csv'\n",
    "    tr_class: level string used to locate the products: [\"citytr\", \"towntr\", \"villagetr\"]\n",
    "    \n",
    "    \"\"\"\n",
    "    url_init, code2, pdt2 = url_cpi\n",
    "    \n",
    "    filename = '_'.join(['pdt_all', code2]) + '.csv'\n",
    "    html = reopen(url_init, maxtr, timeout)\n",
    "    page_sp = soup(html.read(), 'html.parser')\n",
    "    pdt_containers = page_sp.find_all(\"tr\", {\"class\": \"citytr\"})\n",
    "    csvfile = open(filename, 'w', encoding = 'utf-8-sig',  newline = '')\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['code2', 'pdt2', 'code4', 'pdt4', 'code6', 'pdt6', 'code8', 'pdt8', 'code10', 'pdt10'])   # note that this may be consuming a lot of internal memory\n",
    "   \n",
    "    for item in pdt_containers: \n",
    "        item_a, href1 = reftextRetrieve(item)\n",
    "        \n",
    "        if item_a!=[]:\n",
    "            code4, pdt4 = [item_a[0].text, item_a[1].text]\n",
    "            print('Now scraping:' + code4)\n",
    "        if href1 ==[]:\n",
    "            code6, pdt6 = ['', '']\n",
    "            code8, pdt8 = ['', '']\n",
    "            code10,pdt10= ['', ''] \n",
    "            writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "        else:\n",
    "            url1 = url_body + href1   #hyperlinks to subcategories of 6 digit\n",
    "            html1 = reopen(url1, maxtr, timeout)\n",
    "            page_sp1 = soup(html1.read(), 'html.parser')\n",
    "            pdt_containers1 = page_sp1.find_all(\"tr\", {\"class\": \"countytr\"})\n",
    "            if pdt_containers1==[] and page_sp1.find_all(\"tr\", {\"class\": \"villagetr\"})!=[]:\n",
    "                code6, pdt6 = ['', '']\n",
    "                code8, pdt8 = ['', '']\n",
    "                pdt_containers3 = page_sp1.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                for item in pdt_containers3:\n",
    "                    item_a3 = item.select('td')\n",
    "                    code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                    print(code10, pdt10)\n",
    "                    writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "            elif pdt_containers1==[] and page_sp1.find_all(\"tr\", {\"class\": \"towntr\"})!=[]:\n",
    "                pdt_containers2 = page_sp1.find_all(\"tr\", {\"class\": \"towntr\"})\n",
    "                code6, pdt6 = ['', '']\n",
    "                if pdt_containers2!=[]:\n",
    "                    for item in pdt_containers2:\n",
    "                        item_a2, href3 = reftextRetrieve(item)\n",
    "                        if item_a2!=[]:\n",
    "                            code8, pdt8 = [item_a2[0].text, item_a2[1].text]\n",
    "                            if href3 == []:\n",
    "                                code10, pdt10 = ['', '']\n",
    "                                writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "                            else:\n",
    "                                url3 = url_body + '/'.join([code4[0:2], code4[2:4]]) + '/'+ href3\n",
    "                                html3 = reopen(url3, maxtr, timeout)\n",
    "                                page_sp3 = soup(html3.read(), 'html.parser')\n",
    "                                pdt_containers3 = page_sp3.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "\n",
    "                                for item in pdt_containers3:\n",
    "                                        item_a3 = item.select('td')\n",
    "                                        code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                        writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "            else: \n",
    "                #print('url1:' + url1)\n",
    "                for item in pdt_containers1:\n",
    "                    item_a1, href2 = reftextRetrieve(item) # 6-digit items\n",
    "                    if item_a1!=[]:\n",
    "                        code6, pdt6 = [item_a1[0].text, item_a1[1].text]\n",
    "                        if href2==[]:\n",
    "                            code8, pdt8 = ['', '']\n",
    "                            code10,pdt10= ['', '']\n",
    "                            writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])                \n",
    "                        else: \n",
    "                            url2 = url_body + code2 +'/' + href2 # hyperlinks to 8-digit\n",
    "                            html2 = reopen(url2, maxtr, timeout)\n",
    "                            #print(url2)\n",
    "                            page_sp2 = soup(html2.read(), 'html.parser')\n",
    "                            pdt_containers2 = page_sp2.find_all(\"tr\", {\"class\": \"towntr\"})\n",
    "\n",
    "                            if pdt_containers2!=[]:\n",
    "                                for item in pdt_containers2:\n",
    "                                    item_a2, href3 = reftextRetrieve(item)\n",
    "                                    if item_a2!=[]:\n",
    "                                        code8, pdt8 = [item_a2[0].text, item_a2[1].text]\n",
    "                                        if href3 == []:\n",
    "                                            code10, pdt10 = ['', '']\n",
    "                                            writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "                                        else:\n",
    "                                            url3 = url_body + '/'.join([code4[0:2], code4[2:4]]) + '/'+ href3\n",
    "                                            html3 = reopen(url3, maxtr, timeout)\n",
    "                                            page_sp3 = soup(html3.read(), 'html.parser')\n",
    "                                            pdt_containers3 = page_sp3.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "\n",
    "                                            for item in pdt_containers3:\n",
    "                                                    item_a3 = item.select('td')\n",
    "                                                    code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                                    writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "                            else:\n",
    "                                    code8, pdt8 = ['','']\n",
    "                                    pdt_containers3 = page_sp2.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                                    for item in pdt_containers3:\n",
    "                                        item_a3 = item.select('td')\n",
    "                                        code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                        writer.writerow([code2, pdt2, code4, pdt4, code6, pdt6, code8, pdt8, code10, pdt10])\n",
    "        print('Sucess! 4-digit Product:'+ code4) \n",
    "    csvfile.close()\n",
    "    return csvfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now scraping:3701\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.5578148297281733\n",
      "Sucess! 4-digit Product:3701\n",
      "Now scraping:3702\n",
      "Sucess! 4-digit Product:3702\n",
      "Now scraping:3703\n",
      "Sucess! 4-digit Product:3703\n",
      "Now scraping:3704\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.7179158425263839\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.8983016854110115\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.584029891216166\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.3503406324054663\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.3877446154026352\n",
      "Sucess! 4-digit Product:3704\n",
      "Now scraping:3705\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.4668866185866305\n",
      "Sucess! 4-digit Product:3705\n",
      "Now scraping:3706\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.9963684180736943\n",
      "Sucess! 4-digit Product:3706\n",
      "Now scraping:3707\n",
      "Sucess! 4-digit Product:3707\n",
      "Now scraping:3708\n",
      "Sucess! 4-digit Product:3708\n",
      "Now scraping:3709\n",
      "Sucess! 4-digit Product:3709\n",
      "Now scraping:3710\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.5464630360480127\n",
      "Sucess! 4-digit Product:3710\n",
      "Now scraping:3711\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.4895036183220676\n",
      "Sucess! 4-digit Product:3711\n",
      "Now scraping:3712\n",
      "Sucess! 4-digit Product:3712\n",
      "Now scraping:3713\n",
      "Sucess! 4-digit Product:3713\n",
      "Now scraping:3714\n",
      "connection timeout, retrying: 1\n",
      "sleeping time: 1.99372676574421\n",
      "connection timeout, retrying: 2\n",
      "sleeping time: 2.05558567472918\n",
      "connection timeout, retrying: 3\n",
      "sleeping time: 3.819514060473355\n",
      "connection timeout, retrying: 4\n",
      "sleeping time: 4.877836631691459\n",
      "connection timeout, retrying: 5\n",
      "sleeping time: 5.407008009270724\n",
      "connection timeout, retrying: 6\n",
      "sleeping time: 6.43042993117434\n",
      "connection timeout, retrying: 7\n",
      "sleeping time: 7.360517421794361\n",
      "connection timeout, retrying: 8\n",
      "sleeping time: 8.906651405658083\n",
      "connection timeout, retrying: 9\n",
      "sleeping time: 9.995178390007496\n",
      "connection timeout, retrying: 10\n",
      "sleeping time: 10.254856831563565\n",
      "connection timeout, retrying: 11\n",
      "sleeping time: 11.849202393041299\n",
      "connection timeout, retrying: 12\n",
      "sleeping time: 12.736330756756479\n",
      "connection timeout, retrying: 13\n",
      "sleeping time: 13.927690974866506\n",
      "connection timeout, retrying: 14\n",
      "sleeping time: 14.664812499804741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [05:10<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection timeout, retrying: 15\n",
      "sleeping time: 15.53304993389305\n",
      "Dead Url:http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/37/14/01/37140103.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2bc990613038>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'37'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0murl_cpi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0murl_2d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcode_product\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mnbsScrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_cpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d44b5c0f45a0>\u001b[0m in \u001b[0;36mnbsScrap\u001b[1;34m(url_cpi, url_body, maxtr, timeout)\u001b[0m\n\u001b[0;32m     92\u001b[0m                                             \u001b[0murl3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murl_body\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcode4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode4\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mhref3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                                             \u001b[0mhtml3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m                                             \u001b[0mpage_sp3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m                                             \u001b[0mpdt_containers3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpage_sp3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"class\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"villagetr\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "# back up codes when the scraping fails \n",
    "# this is triggered mannually if the previous program encounters error\n",
    "# Failures: 32, 36, 37\n",
    "id = n\n",
    "index_url = index_urls[id]\n",
    "html = reopen(index_url, maxtr, timeout)\n",
    "page_soup = soup(html.read(), 'html.parser')\n",
    "containers = page_soup.findAll(\"span\", {'class':'cont_tit'})\n",
    "code_product = []\n",
    "for item in containers:\n",
    "    item_list = item.findAll('font', {'class':'cont_tit03'})\n",
    "    name = item_list[0].text\n",
    "    code_product.append([name[0:2], name[3:]])  #find all the two digit products\n",
    "\n",
    "tags = [code_product[i][0] for i in range(len(code_product))] # tags for html links to 4-digit\n",
    "url_2d = []   # array of links to 4-digit\n",
    "for tag in tags:\n",
    "    url_2d.append(tag.join(['http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/', '.html']))\n",
    "id_fail = i\n",
    "\n",
    "for j in tqdm(range(tags.index('37'), len(tags))):\n",
    "    url_cpi = [url_2d[j]] + code_product[j]\n",
    "    nbsScrap(url_cpi, url_body, maxtr, timeout) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
