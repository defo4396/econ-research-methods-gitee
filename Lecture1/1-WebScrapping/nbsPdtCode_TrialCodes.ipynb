{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBS 产品爬虫--Trial Codes\n",
    "``By Zhiyuan Chen``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of retrying requesting the website\n",
    "import random\n",
    "import urllib.request\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "\n",
    "def reopen(url, timeout, maxt):\n",
    "    \"\"\"\n",
    "    This function reopens the url under random sleeping time, the sleep time increases\n",
    "    as more trials are called \n",
    "    ---------------------------------------------------------------------------------\n",
    "    Var Def:\n",
    "    url: objective url\n",
    "    timeout: maximum time allowed for opening the url\n",
    "    longsleep: the time for a long sleep after failed trials\n",
    "    maxt: maximum number of trials\n",
    "    maxpause: maximum number of pauses (long sleep)\n",
    "    \"\"\"\n",
    "    fails = 0\n",
    "    pause = 0\n",
    "    while True:\n",
    "        try:\n",
    "            if fails >= maxt:\n",
    "                #if pause >= maxpause:\n",
    "                #    print('Dead Url:'+url)\n",
    "                #    print('Number of Pauses:', maxpause)\n",
    "                #    break\n",
    "                #print('Getting a pause of ', longsleep,'s...')\n",
    "                #time.sleep(longsleep + random.random())\n",
    "                #pause +=1\n",
    "                #html = opener.open(url, None, timeout)\n",
    "                #return html\n",
    "                print('Dead Url:'+url)\n",
    "                break\n",
    "            html = opener.open(url, None, timeout)\n",
    "            return html\n",
    "        except:\n",
    "            fails +=1\n",
    "            stime = fails + random.random() # sleeping time increases as we try more\n",
    "            time.sleep(stime)\n",
    "            print ('connection timeout, retrying:', fails)\n",
    "            print ('sleeping time:', stime)\n",
    "        else: \n",
    "            print(url)\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function of retrieving hrefs and texts\n",
    "#-------------------------------------------------\n",
    "def reftextRetrieve(item):\n",
    "    if item.select('a')!=[]:\n",
    "        item_a = item.select('a')\n",
    "        href = item_a[0].get('href')\n",
    "        return item_a, href\n",
    "    else:\n",
    "        if item.select('td')!=[]:\n",
    "            item_a = item.select('td')\n",
    "            href = []\n",
    "            return item_a, href\n",
    "        else:\n",
    "            print('EEOR!: selected invalid tag') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the first page with two-digit products\n",
    "\n",
    "my_url = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/index.html'\n",
    "html = uReq(my_url)\n",
    "page_soup = soup(html.read(), 'html.parser')\n",
    "\n",
    "# find each name of the product catalog\n",
    "containers = page_soup.findAll(\"span\", {\"class\":\"cont_tit\"})\n",
    "code_product = []\n",
    "for item in containers:\n",
    "    item_list = item.findAll('font', {'class':'cont_tit03'})\n",
    "    name = item_list[0].text\n",
    "    code_product.append([name[0:2], name[3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code10:0119010000\n",
      "code10:0119020000\n",
      "code10:0119030000\n",
      "code10:0119040000\n",
      "code10:0119050000\n",
      "code10:0119060000\n",
      "code10:0119070000\n",
      "code10:0119080000\n",
      "code10:0119090000\n",
      "code10:0119100000\n",
      "code10:0119110000\n",
      "code10:0119120000\n",
      "code10:0119130000\n",
      "code10:0119140000\n",
      "code10:0119150000\n",
      "code10:0119160000\n",
      "code10:0119170000\n",
      "code10:0119180000\n",
      "code10:0119190000\n",
      "code10:0119200000\n",
      "code10:0119210000\n",
      "code10:0119220000\n",
      "code10:0119230000\n",
      "code10:0119240000\n",
      "code10:0119250000\n",
      "code10:0119260000\n",
      "code10:0119270000\n",
      "code10:0119280000\n",
      "code10:0119290000\n",
      "code10:0119300000\n",
      "code10:0119310000\n",
      "code10:0119320000\n",
      "code10:0119330000\n",
      "code10:0119340000\n",
      "code10:0119350000\n",
      "code10:0119360000\n",
      "code10:0119370000\n",
      "code10:0119380000\n",
      "code10:0119390000\n",
      "code10:0119400000\n",
      "code10:0119410000\n",
      "code10:0119420000\n",
      "code10:0119430000\n",
      "code10:0119440000\n",
      "code10:0119450000\n",
      "code10:0119460000\n",
      "code10:0119470000\n",
      "code10:0119480000\n",
      "code10:0119490000\n",
      "code10:0119990000\n"
     ]
    }
   ],
   "source": [
    "# reopen if timeout\n",
    "# random sleeping time after each trial\n",
    "# a long pause after a sequence of trials\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "t0 = time.time()\n",
    "# settings for the reopen() function\n",
    "#------------------------------------------------------\n",
    "headers =  ('User-Agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11')\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders  = [headers]\n",
    "maxtr = 10\n",
    "timeout= 10\n",
    "\n",
    "#------------------------------------------------------\n",
    "# index page 1\n",
    "tags_index = [code_product[i][0] for i in range(len(code_product))] # tags for index html links\n",
    "url_index = []\n",
    "html_main = 'http://www.stats.gov.cn/tjsj/tjbz/tjypflml/2010/'\n",
    "for tag in tags_index:\n",
    "    link = html_main + tag +'.html'\n",
    "    url_index.append(link)\n",
    "\n",
    "#Page 1#\n",
    "for i, url in enumerate([url_index[0]]):\n",
    "    code2, pdt2 = code_product[i]  # two-digit products\n",
    "    html = reopen(url, maxtr, timeout)\n",
    "    page_sp = soup(html.read(), 'html.parser')\n",
    "    pdt_containers = page_sp.find_all(\"tr\", {\"class\": \"citytr\"})\n",
    "\n",
    "id = 18\n",
    "item = pdt_containers[id]\n",
    "item_a, href1 = reftextRetrieve(item)\n",
    "\n",
    "if item_a!=[]:\n",
    "   code4, pdt4 = [item_a[0].text, item_a[1].text]\n",
    "   if href1 ==[]:\n",
    "      code6, pdt6 = ['', '']\n",
    "      code8, pdt8 = ['', '']\n",
    "      code10,pdd10= ['', ''] \n",
    "      \n",
    "   else:\n",
    "       url1 = html_main + href1 #hyperlinks to subcategories of 6 digit\n",
    "       html1 = reopen(url1, maxtr, timeout)\n",
    "       page_sp1 = soup(html1.read(), 'html.parser')\n",
    "       pdt_containers1 = page_sp1.find_all(\"tr\", {\"class\": \"countytr\"})\n",
    "       for item in pdt_containers1:\n",
    "            item_a1, href2 = reftextRetrieve(item)\n",
    "            if item_a1!=[]:\n",
    "               code6, pdt6 = [item_a1[0].text, item_a1[1].text]\n",
    "               if href2==[]:\n",
    "                  code8, pdt8 = ['', '']\n",
    "                  code10,pdd10= ['', '']\n",
    "                                 \n",
    "               else: \n",
    "                   url2 = html_main + code2 +'/' + href2 # hyperlinks to 8-digit\n",
    "                   html2 = reopen(url2, maxtr, timeout)\n",
    "                   page_sp2 = soup(html2.read(), 'html.parser')\n",
    "                   pdt_containers2 = page_sp2.find_all(\"tr\", {\"class\": \"towntr\"})\n",
    "                   if pdt_containers2!=[]:\n",
    "                       for item in pdt_containers2:\n",
    "                           item_a2, href3 = reftextRetrieve(item)\n",
    "                           if item_a2!=[]:\n",
    "                               code8, pdt8 = [item_a2[0].text, item_a2[1].text]\n",
    "                               if href2 == []:\n",
    "                                 code10, pdt10 = ['', '']\n",
    "                                 \n",
    "                               else:\n",
    "                                  url3 = html_main + '/'.join([code4[0:2], code4[2:4]]) + '/'+ href3\n",
    "                                  html3 = reopen(url3, maxtr, timeout)\n",
    "                                  page_sp3 = soup(html3.read(), 'html.parser')\n",
    "                                  pdt_containers3 = page_sp3.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                                  for item in pdt_containers3:\n",
    "                                        item_a3 = item.select('td')\n",
    "                                        code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                                        print('code10:' + code10)\n",
    "                                        \n",
    "                   else:\n",
    "                        code8, pdt8 = ['','']\n",
    "                        pdt_containers3 = page_sp2.find_all(\"tr\", {\"class\": \"villagetr\"})\n",
    "                        for item in pdt_containers3:\n",
    "                            item_a3 = item.select('td')\n",
    "                            code10, pdt10 = [item_a3[0].text, item_a3[1].text]\n",
    "                            print('code10:' + code10)\n",
    "                            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}